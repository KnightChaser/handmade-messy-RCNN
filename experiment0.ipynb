{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RCNN experiment\n",
        "\n",
        "#### My Environment\n",
        "```text\n",
        "            .-/+oossssoo+/-.               lixe@phtmaiden \n",
        "        `:+ssssssssssssssssss+:`           -------------- \n",
        "      -+ssssssssssssssssssyyssss+-         OS: Ubuntu 24.04.2 LTS x86_64 \n",
        "    .ossssssssssssssssssdMMMNysssso.       Host: ROG Strix G733ZS_G733ZS 1.0 \n",
        "   /ssssssssssshdmmNNmmyNMMMMhssssss/      Kernel: 6.11.0-26-generic \n",
        "  +ssssssssshmydMMMMMMMNddddyssssssss+     Uptime: 13 hours, 14 mins \n",
        " /sssssssshNMMMyhhyyyyhmNMMMNhssssssss/    Packages: 2180 (dpkg), 19 (snap) \n",
        ".ssssssssdMMMNhsssssssssshNMMMdssssssss.   Shell: bash 5.2.21 \n",
        "+sssshhhyNMMNyssssssssssssyNMMMysssssss+   Resolution: 2560x1440 \n",
        "ossyNMMMNyMMhsssssssssssssshmmmhssssssso   DE: GNOME 46.0 \n",
        "ossyNMMMNyMMhsssssssssssssshmmmhssssssso   WM: Mutter \n",
        "+sssshhhyNMMNyssssssssssssyNMMMysssssss+   WM Theme: Adwaita \n",
        ".ssssssssdMMMNhsssssssssshNMMMdssssssss.   Theme: WhiteSur-Light-solid [GTK2/3] \n",
        " /sssssssshNMMMyhhyyyyhdNMMMNhssssssss/    Icons: WhiteSur [GTK2/3] \n",
        "  +sssssssssdmydMMMMMMMMddddyssssssss+     Terminal: xfce4-terminal \n",
        "   /ssssssssssshdmNNNNmyNMMMMhssssss/      Terminal Font: Monospace 12 \n",
        "    .ossssssssssssssssssdMMMNysssso.       CPU: 12th Gen Intel i9-12900H (20) @ 4.900GHz \n",
        "      -+sssssssssssssssssyyyssss+-         GPU: NVIDIA GeForce RTX 3080 Mobile / Max-Q 8GB/16GB \n",
        "        `:+ssssssssssssssssss+:`           GPU: Intel Alder Lake-P GT2 [Iris Xe Graphics] \n",
        "             .-/+oossssoo+/-.              Memory: 13480MiB / 63959MiB \n",
        "```\n",
        "\n",
        "VOC data from http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
        "Unzip this data and store this as `VOCdevkit` like below setup \n",
        "```text\n",
        "lixe@phtmaiden:~/RCNNLocalTest$ ls -al .\n",
        "total 2776\n",
        "drwxrwxr-x  8 lixe lixe    4096 Jun  3 00:04 .\n",
        "drwxr-x--- 39 lixe lixe    4096 Jun  3 00:04 ..\n",
        "-rw-rw-r--  1 lixe lixe 2799698 Jun  2 22:54 experiment0.ipynb\n",
        "drwxrwxr-x  7 lixe lixe    4096 Jun  3 00:04 .git\n",
        "-rw-rw-r--  1 lixe lixe      36 Jun  3 00:04 .gitignore\n",
        "drwxrwxr-x  2 lixe lixe    4096 Jun  1 14:44 __pycache__\n",
        "drwxrwxr-x  4 lixe lixe    4096 Jun  2 22:54 RCNNDataCache\n",
        "-rw-rw-r--  1 lixe lixe    1451 Jun  1 14:17 requirements.txt\n",
        "drwxrwxr-x  2 lixe lixe    4096 Jun  2 00:18 screenshots\n",
        "drwxrwxr-x  6 lixe lixe    4096 May 31 21:40 .venv\n",
        "drwxrwxr-x  3 lixe lixe    4096 Jun  1 15:24 VOCdevkit <---- Downloaded VOC 2007 data (unzipped)\n",
        "```\n",
        "\n",
        "The VOC data looks like below.\n",
        "```text\n",
        "VOCdevkit\n",
        "  VOC2007\n",
        "    Annotations\n",
        "    ImageSets\n",
        "    JPEGImages\n",
        "    SegmentationClass\n",
        "    SegmentationObject\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2nqAG2y_14j",
        "outputId": "05b87e48-6b84-4b53-be73-6f0364b8820a"
      },
      "outputs": [],
      "source": [
        "%pip install torch torchvision scikit-learn opencv-python lxml matplotlib\n",
        "%pip install cython pycocotools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2ZxSFjHIn82",
        "outputId": "a59347c5-aa57-47c4-b3de-557f8fa934b4"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import VOCDetection\n",
        "\n",
        "# Load the VOC dataset using the unzipped directory\n",
        "voc_dataset = VOCDetection(\n",
        "    root='.', # Root is now the parent directory of VOCdevkit\n",
        "    year='2007',\n",
        "    image_set='trainval',\n",
        "    download=False, # Set download to False since we are loading from a local file\n",
        "    transform=None,\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "print(\"VOC dataset loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 997
        },
        "id": "vhpZ2-gRDzmy",
        "outputId": "517416aa-6a2e-4fd8-dbf7-66c04a67f80f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the number of images to display in a grid\n",
        "num_images_to_display = 9\n",
        "grid_size = int(num_images_to_display**0.5)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "for i in range(num_images_to_display):\n",
        "    # Only print images :)\n",
        "    img, _ = voc_dataset[i]\n",
        "\n",
        "    plt.subplot(grid_size, grid_size, i + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(f'Image {i+1}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwN0PeyXE3F0"
      },
      "source": [
        "### Generate region proposals with selective search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOj70WTjEQwV",
        "outputId": "e755f229-4f4f-4dc6-f489-9764bd7d124f"
      },
      "outputs": [],
      "source": [
        "%pip install opencv-contrib-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_cYt5dsE2F-"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "JPAmd5Z3Jj8G",
        "outputId": "03400326-2e09-4490-a168-4573e6278319"
      },
      "outputs": [],
      "source": [
        "image_path = './VOCdevkit/VOC2007/JPEGImages/000001.jpg'\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Ensure image is loaded\n",
        "if image is None:\n",
        "    print(f\"Error loading image: {image_path}\")\n",
        "else:\n",
        "    ss.setBaseImage(image)\n",
        "    ss.switchToSelectiveSearchFast()            # or switchToSelectiveSearchQuality()\n",
        "    rects = ss.process()                        # list of (x, y, w, h)\n",
        "    proposals = rects[:2000]                    # keep top 2000 (for speed/consistency)\n",
        "\n",
        "    # Filter out small proposals\n",
        "    proposals = [p for p in proposals if p[2] >= 20 and p[3] >= 20]\n",
        "\n",
        "    # Create a copy of the image to draw rectangles on\n",
        "    image_with_proposals = image.copy()\n",
        "\n",
        "    # Draw the top N proposals on the image\n",
        "    num_proposals_to_show = 50 # Adjust this number to show more or fewer proposals\n",
        "    for i, rect in enumerate(proposals[:num_proposals_to_show]):\n",
        "        x, y, w, h = rect\n",
        "        cv2.rectangle(image_with_proposals, (x, y), (x + w, y + h), (0, 255, 0), 2) # Draw a green rectangle\n",
        "\n",
        "    # Convert the image from BGR to RGB for displaying with matplotlib\n",
        "    image_with_proposals_rgb = cv2.cvtColor(image_with_proposals, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Display the image with proposals\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(image_with_proposals_rgb)\n",
        "    plt.title(f'Image with Top {num_proposals_to_show} Region Proposals')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHlp6rlQJukr"
      },
      "source": [
        "### Warp/Crop and save images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjM4WhO7Jrtq"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def warp_and_save(image, box: tuple[int,int,int,int], output_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    Crop a region from `image` and resize to 227x227, then save to `output_path`.\n",
        "    Returns True if saved successfully, False otherwise.\n",
        "    \"\"\"\n",
        "    x, y, w, h = box\n",
        "    # 1) Crop via NumPy slicing\n",
        "    crop = image[y : y + h, x : x + w]\n",
        "\n",
        "    # 2) If crop is empty or invalid, skip saving\n",
        "    if crop is None or crop.shape[0] == 0 or crop.shape[1] == 0:\n",
        "        return False\n",
        "\n",
        "    # 3) Resize to 227×227 (according to the R-CNN paper!)\n",
        "    resized = cv2.resize(crop, (227, 227), interpolation = cv2.INTER_LINEAR)\n",
        "\n",
        "    # 4) Save with cv2.imwrite\n",
        "    success = cv2.imwrite(output_path, resized)\n",
        "    return success\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGWgmFwpPufg"
      },
      "source": [
        "Get all available image IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CbpsWo3NQqi",
        "outputId": "a31da179-a030-4731-db97-21af9b0d2973"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "# 1) Define directory path\n",
        "jpg_dir = './VOCdevkit/VOC2007/JPEGImages'\n",
        "\n",
        "# 2) List all entries and filter for .jpg files\n",
        "all_files = os.listdir(jpg_dir)\n",
        "jpg_filenames = [f for f in all_files if f.lower().endswith('.jpg')]\n",
        "\n",
        "# 3) Strip extensions to get base names\n",
        "base_names = [os.path.splitext(fname)[0] for fname in jpg_filenames]\n",
        "\n",
        "# 4) Use regex to extract numeric portions\n",
        "regex = re.compile(r'\\d+')\n",
        "numeric_ids = []\n",
        "for name in base_names:\n",
        "    m = regex.search(name)\n",
        "    if m:\n",
        "        numeric_ids.append(int(m.group()))\n",
        "# If numeric_ids is empty, raise an error\n",
        "if not numeric_ids:\n",
        "    raise RuntimeError(f\"No numeric filenames found in {jpg_dir}\")\n",
        "\n",
        "# 5) Find min and max\n",
        "min_id = min(numeric_ids)\n",
        "max_id = max(numeric_ids)\n",
        "\n",
        "print(f\"Numeric ID range discovered: {min_id} to {max_id}\")\n",
        "\n",
        "# 6) Reconstruct zero-padded strings (assuming 6 digits total)\n",
        "num_digits = 6\n",
        "all_ids_in_range = [f\"{i:0{num_digits}d}\" for i in range(min_id, max_id + 1)]\n",
        "\n",
        "# 7) Filter to those that actually exist on disk\n",
        "existing_ids = set(base_names)\n",
        "trainval_ids = [img_id for img_id in all_ids_in_range if img_id in existing_ids]\n",
        "\n",
        "print(f\"Total valid img_id strings found: {len(trainval_ids)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7Yjav8TPyWy"
      },
      "source": [
        "Get all possible proposal regions as a Python3 dictionary form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBD4b5llTu97"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Create a global SS object so that each worker process can initialize it once\n",
        "_ss_global = None\n",
        "\n",
        "def init_ss():\n",
        "    global _ss_global\n",
        "    _ss_global = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
        "\n",
        "def compute_proposals_for_image(img_id):\n",
        "    \"\"\"\n",
        "    Worker function for multiprocessing.\n",
        "    Loads image, runs Selective Search, returns a tuple: (img_id, filtered_boxes).\n",
        "    \"\"\"\n",
        "    global _ss_global\n",
        "    # Load image\n",
        "    img_path = f'./VOCdevkit/VOC2007/JPEGImages/{img_id}.jpg'\n",
        "    image = cv2.imread(img_path)\n",
        "    if image is None:\n",
        "        return (img_id, [])  # skip if missing\n",
        "\n",
        "    # Configure and run SS\n",
        "    _ss_global.setBaseImage(image)\n",
        "    _ss_global.switchToSelectiveSearchFast()\n",
        "    rects = _ss_global.process()\n",
        "    top_rects = rects[:300]  # Keep 300 out of 2,000 top proposals.\n",
        "    # NOTE: I tried 50, but it was too poor. mAP is 0.04 ~ 0.1. What the f***?????\n",
        "    # NOTE: I tried 500, then my computer crashed.\n",
        "\n",
        "    # Filter out tiny ones\n",
        "    filtered = [r for r in top_rects if r[2] >= 20 and r[3] >= 20]\n",
        "    return (img_id, filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Building proposal regions. It consumes CPU due to the package's internal logic so I utilized parallelism and (really, you won't truly understand my anger and frustration) switched the environment from Google colab to my lovely laptop with i9-12900 CPU.\n",
        "\n",
        "![image_region_proposal_screenshot.png](./screenshots/image_region_proposal_screenshot.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC1hep5YT1od",
        "outputId": "f5410eeb-e015-4b5e-d6e5-d0e820a7cc0f"
      },
      "outputs": [],
      "source": [
        "import multiprocessing as mp\n",
        "import glob\n",
        "import os\n",
        "\n",
        "num_workers = mp.cpu_count() - 1 or 1  # don’t lock up every core\n",
        "with mp.Pool(processes=num_workers, initializer=init_ss) as pool:\n",
        "    results = []\n",
        "    for idx, res in enumerate(pool.imap(compute_proposals_for_image, trainval_ids), 1):\n",
        "        results.append(res)\n",
        "        print(f\"[{idx / len(trainval_ids) * 100:.2f}%] {idx} / {len(trainval_ids)} images processed\")\n",
        "\n",
        "# Merge with any previously processed results if needed\n",
        "proposals_dict = {img_id: boxes for (img_id, boxes) in results}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the proposals for the first 5 images\n",
        "for img_id in list(proposals_dict.keys())[:5]:\n",
        "    print(f\"Image ID: {img_id}, Proposals: {len(proposals_dict[img_id])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iSSS_JJNgcE"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "os.makedirs('./RCNNDataCache/warped_patches', exist_ok=True)\n",
        "\n",
        "for img_id, boxes in tqdm(proposals_dict.items(), desc=\"Saving warped patches\"):\n",
        "    path = f'./VOCdevkit/VOC2007/JPEGImages/{img_id}.jpg'\n",
        "    image = cv2.imread(path)\n",
        "    if image is None:\n",
        "        print(f\"Error loading image: {path}\")\n",
        "        continue\n",
        "    for i, box in enumerate(boxes):\n",
        "        out_name = f'{img_id}_{i}.jpg'\n",
        "        out_path = os.path.join('./RCNNDataCache/warped_patches', out_name)\n",
        "        if os.path.exists(out_path):\n",
        "            continue  # Skip if file already exists\n",
        "        warp_and_save(image, box, out_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX2OEMCTMm6f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Display some of the saved patches to verify they were created correctly\n",
        "patch_filenames = [\n",
        "    './RCNNDataCache/warped_patches/000005_0.jpg',\n",
        "    './RCNNDataCache/warped_patches/000005_1.jpg',\n",
        "    './RCNNDataCache/warped_patches/000005_2.jpg',\n",
        "    './RCNNDataCache/warped_patches/000005_3.jpg',\n",
        "    './RCNNDataCache/warped_patches/000005_4.jpg',\n",
        "    './RCNNDataCache/warped_patches/000005_5.jpg',\n",
        "    './RCNNDataCache/warped_patches/000005_6.jpg',\n",
        "    './RCNNDataCache/warped_patches/000005_7.jpg',\n",
        "    './RCNNDataCache/warped_patches/000005_8.jpg'\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "for i, fname in enumerate(patch_filenames):\n",
        "    patch = cv2.imread(fname)\n",
        "    patch_rgb = cv2.cvtColor(patch, cv2.COLOR_BGR2RGB)\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(patch_rgb)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Patch {i+1}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train models..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import joblib  # for saving/loading scikit-learn models\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define class names and label mapping\n",
        "VOC_CLASSES = [\n",
        "    'aeroplane',\n",
        "    'bicycle',\n",
        "    'bird',\n",
        "    'boat',\n",
        "    'bottle',\n",
        "    'bus',\n",
        "    'car',\n",
        "    'cat',\n",
        "    'chair',\n",
        "    'cow',\n",
        "    'diningtable',\n",
        "    'dog',\n",
        "    'horse',\n",
        "    'motorbike',\n",
        "    'person',\n",
        "    'pottedplant',\n",
        "    'sheep',\n",
        "    'sofa',\n",
        "    'train',''\n",
        "    'tvmonitor'\n",
        "]\n",
        "CLASS_TO_IDX = {cls_name: idx for idx, cls_name in enumerate(VOC_CLASSES)}\n",
        "\n",
        "# Custom Dataset\n",
        "class RCNNDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset to load warped proposal patches and corresponding labels for fine-tuning.\n",
        "    Expects a CSV-like index file where each row contains:\n",
        "      img_id (str), proposal_idx (int), label (int), x, y, w, h\n",
        "    \"\"\"\n",
        "    def __init__(self, index_file, patches_dir, transforms=None):\n",
        "        \"\"\"\n",
        "        index_file: path to a text file or CSV listing every patch and its label/box.\n",
        "        patches_dir: directory where warped patches (227x227) are stored.\n",
        "        transforms: torchvision transforms to apply to each patch.\n",
        "        \"\"\"\n",
        "        # Read lines: each line: img_id,proposal_idx,label,x,y,w,h\n",
        "        with open(index_file, 'r') as f:\n",
        "            lines = [line.strip().split(',') for line in f if line.strip()]\n",
        "        self.entries = []\n",
        "        for tokens in lines:\n",
        "            img_id, prop_idx, label, x, y, w, h = tokens\n",
        "            self.entries.append({\n",
        "                'img_id': img_id,\n",
        "                'prop_idx': int(prop_idx),\n",
        "                'label': int(label),\n",
        "                'bbox': (int(x), int(y), int(w), int(h))\n",
        "            })\n",
        "        self.patches_dir = patches_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ent = self.entries[idx]\n",
        "        img_id = ent['img_id']\n",
        "        prop_idx = ent['prop_idx']\n",
        "        label = ent['label']\n",
        "        \n",
        "        # Build path: e.g., '000005_12.jpg'\n",
        "        fname = f\"{img_id}_{prop_idx}.jpg\"\n",
        "        img_path = os.path.join(self.patches_dir, fname)\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            raise FileNotFoundError(f\"Patch not found: {img_path}\")\n",
        "        \n",
        "        # Convert BGR -> RGB, then to PIL for torchvision transforms\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Convert to PIL.Image\n",
        "        image = transforms.ToPILImage()(image)\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "        return image, label, ent['bbox'], img_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate `index_file.txt` Containing (img_id, proposal_idx, label, x, y, w, h).\n",
        "We need to scan through every `img_id` in `trainval_ids`, and for each of its proposals (from `proposals_dict`), compute IoU with ground-truth boxes (in VOC XML) to decide if it’s a positive (label=c) or a negative (label=20 as “background”).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def parse_voc_annotation(xml_path):\n",
        "    \"\"\"\n",
        "    Parse a VOC XML annotation and return a list of dicts:\n",
        "        [{'class': 'person', 'bbox': (xmin, ymin, xmax, ymax)}, ...]\n",
        "    \"\"\"\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    objs = []\n",
        "    for obj in root.findall('object'):\n",
        "        cls_name = obj.find('name').text\n",
        "        xmin = int(obj.find('bndbox/xmin').text)\n",
        "        ymin = int(obj.find('bndbox/ymin').text)\n",
        "        xmax = int(obj.find('bndbox/xmax').text)\n",
        "        ymax = int(obj.find('bndbox/ymax').text)\n",
        "        objs.append({'class': cls_name, 'bbox': (xmin, ymin, xmax, ymax)})\n",
        "    return objs\n",
        "\n",
        "def compute_iou(boxA, boxB):\n",
        "    \"\"\"\n",
        "    Compute IoU between two boxes in (xmin,ymin,xmax,ymax) format.\n",
        "    \"\"\"\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "    interW = max(0, xB - xA + 1)# Visualize training loss and accuracy\n",
        "    interH = max(0, yB - yA + 1)\n",
        "    interArea = interW * interH\n",
        "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
        "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
        "    unionArea = boxAArea + boxBArea - interArea\n",
        "    if unionArea == 0:\n",
        "        return 0.0\n",
        "    return interArea / unionArea\n",
        "\n",
        "# Path definitions\n",
        "annotations_dir = './VOCdevkit/VOC2007/Annotations'\n",
        "patches_dir = './RCNNDataCache/warped_patches'\n",
        "index_file = './RCNNDataCache/rcnn_index.txt'\n",
        "\n",
        "# Build the index file\n",
        "with open(index_file, 'w') as idx_f:\n",
        "    # Write header (optional)\n",
        "    # idx_f.write('img_id,prop_idx,label,x,y,w,h\\n')\n",
        "    for img_id in trainval_ids:\n",
        "        # Load GT boxes for this image\n",
        "        xml_path = os.path.join(annotations_dir, f\"{img_id}.xml\")\n",
        "        gt_objects = parse_voc_annotation(xml_path)\n",
        "\n",
        "        # Convert GT boxes to (xmin,ymin,xmax,ymax)\n",
        "        gt_boxes = []\n",
        "        gt_labels = []  # parallel list of class indices\n",
        "        for obj in gt_objects:\n",
        "            xmin, ymin, xmax, ymax = obj['bbox']\n",
        "            gt_boxes.append((xmin, ymin, xmax, ymax))\n",
        "            gt_labels.append(CLASS_TO_IDX[obj['class']])\n",
        "\n",
        "        # Iterate proposals\n",
        "        for prop_idx, (x, y, w, h) in enumerate(proposals_dict[img_id]):\n",
        "            # Convert proposal to (xmin,ymin,xmax,ymax)\n",
        "            prop_box = (x, y, x + w, y + h)\n",
        "\n",
        "            # Determine IoU with all GT boxes\n",
        "            best_iou = 0.0\n",
        "            best_label = 20  # default to background index\n",
        "            for gt_box, gt_label in zip(gt_boxes, gt_labels):\n",
        "                iou = compute_iou(prop_box, gt_box)\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_label = gt_label\n",
        "\n",
        "            # Label criteria: IoU > 0.5 => positive class\n",
        "            #                 IoU < 0.3 => negative class\n",
        "            if best_iou >= 0.5:\n",
        "                label = best_label\n",
        "            elif best_iou < 0.3:\n",
        "                label = 20  # background\n",
        "            else:\n",
        "                # Ambiguous: IoU in [0.3, 0.5), skip this proposal\n",
        "                continue\n",
        "\n",
        "            # Write a line: img_id,prop_idx,label,x,y,w,h\n",
        "            idx_f.write(f\"{img_id},{prop_idx},{label},{x},{y},{w},{h}\\n\")\n",
        "\n",
        "# The data will be look like...\n",
        "# 000005,0,20,367,223,67,80\n",
        "# 000005,1,20,163,161,55,43\n",
        "# 000005,2,20,256,119,90,66\n",
        "# 000005,3,20,369,345,65,26\n",
        "# 000005,4,20,450,137,44,49\n",
        "# 000005,5,20,103,130,64,78\n",
        "# 000005,6,8,281,191,44,34"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set Up Image Transforms and Create `DataLoader` for Fine-Tuning\n",
        "\n",
        "We resize (again! we've already done in the previous ipynb cell.) to 227×227 to be safe, convert to tensor, and normalize using ImageNet’s mean/std since AlexNet was pretrained on ImageNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define transforms exactly as R-CNN used (AlexNet normalization)\n",
        "# Ref: https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2\n",
        "alexnet_mean = [0.485, 0.456, 0.406]\n",
        "alexnet_std  = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((227, 227)),               # Already 227×227, but ensure consistency\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=alexnet_mean, std=alexnet_std)\n",
        "])\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "train_dataset = RCNNDataset(\n",
        "    index_file='./RCNNDataCache/rcnn_index.txt',\n",
        "    patches_dir=patches_dir,\n",
        "    transforms=train_transforms\n",
        ")\n",
        "\n",
        "batch_size = 64  # adjust to fit your GPU/CPU memory\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=8,    # number of CPU workers for preprocessing\n",
        "    pin_memory=True   # if using CUDA\n",
        ")\n",
        "\n",
        "print(f\"Num training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of batches per epoch: {len(train_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load pretrained CNN and modify final layer\n",
        "\n",
        "Now Ioad a pretrained AlexNet, then change its classifier’s last `Linear(4096=>1000)` to `Linear(4096=>11)` (20 VOC classes(ref.; above) + 1 background(default)). Do you hate pretrained network? But you must not that I'm not an A.I. researcher like you, too! Like many fields do, you need a leniency like approximation algorithms, I dare to say! >_<"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pretrained AlexNet\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "\n",
        "# Replace the last classifier layer\n",
        "num_ftrs = alexnet.classifier[6].in_features  # 4096\n",
        "alexnet.classifier[6] = nn.Linear(num_ftrs, 21)  # 20 classes + 1 background\n",
        "\n",
        "# Move to device and set to train mode\n",
        "alexnet = alexnet.to(device)\n",
        "alexnet.train()\n",
        "\n",
        "print(alexnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fine-Tune AlexNet for one epoch (classification only)\n",
        "\n",
        "We use a cross-entropy(CE) loss over 21 classes(20 VoCs + 1 backgronds). We only train for a few epochs (e.g., 10–15) as R-CNN did, using a lower learning rate on the base layers. \n",
        "\n",
        "(For 500 proposals, it took 160 minutes 33.3 seconds on my laptop.)\n",
        "\n",
        "The output looks like...\n",
        "```text\n",
        "Epoch [1/10], Batch [50/22280], Loss: 0.1690\n",
        "Epoch [1/10], Batch [100/22280], Loss: 0.0519\n",
        "Epoch [1/10], Batch [150/22280], Loss: 0.2042\n",
        "Epoch [1/10], Batch [200/22280], Loss: 0.3789\n",
        "Epoch [1/10], Batch [250/22280], Loss: 0.1435\n",
        "Epoch [1/10], Batch [300/22280], Loss: 0.0913\n",
        "Epoch [1/10], Batch [350/22280], Loss: 0.0698\n",
        "Epoch [1/10], Batch [400/22280], Loss: 0.1786\n",
        "Epoch [1/10], Batch [450/22280], Loss: 0.1805\n",
        "Epoch [1/10], Batch [500/22280], Loss: 0.1294\n",
        "Epoch [1/10], Batch [550/22280], Loss: 0.0181\n",
        "Epoch [1/10], Batch [600/22280], Loss: 0.0620\n",
        "Epoch [1/10], Batch [650/22280], Loss: 0.1124\n",
        "Epoch [1/10], Batch [700/22280], Loss: 0.0151\n",
        "Epoch [1/10], Batch [750/22280], Loss: 0.0769\n",
        "Epoch [1/10], Batch [800/22280], Loss: 0.1939\n",
        "Epoch [1/10], Batch [850/22280], Loss: 0.0443\n",
        "Epoch [1/10], Batch [900/22280], Loss: 0.3394\n",
        "Epoch [1/10], Batch [950/22280], Loss: 0.3026\n",
        "Epoch [1/10], Batch [1000/22280], Loss: 0.0204\n",
        "Epoch [1/10], Batch [1050/22280], Loss: 0.1290\n",
        "Epoch [1/10], Batch [1100/22280], Loss: 0.0174\n",
        "Epoch [1/10], Batch [1150/22280], Loss: 0.0857\n",
        "Epoch [1/10], Batch [1200/22280], Loss: 0.2984\n",
        "Epoch [1/10], Batch [1250/22280], Loss: 0.1766\n",
        "...\n",
        "Epoch [10/10], Batch [22200/22280], Loss: 0.0021\n",
        "Epoch [10/10], Batch [22250/22280], Loss: 0.0061\n",
        "Epoch 10 finished. Loss: 0.0303, Accuracy: 99.11%\n",
        "Saved fine-tuned AlexNet weights to './RCNNDataCache/alexnet_finetuned.pth'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "finetuned_model_path = './RCNNDataCache/alexnet_finetuned.pth'\n",
        "does_graph_exist = False\n",
        "\n",
        "if os.path.exists(finetuned_model_path):\n",
        "    does_graph_exist = True\n",
        "    print(f\"Fine-tuned model already exists at '{finetuned_model_path}'. Skipping training loop.\")\n",
        "else:\n",
        "    # Define Loss and Optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD([\n",
        "        {'params': alexnet.features.parameters(), 'lr': 1e-4},    # lower LR for conv layers\n",
        "        {'params': alexnet.classifier.parameters(), 'lr': 1e-3}   # higher LR for fc layers\n",
        "    ], momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    num_epochs = 10  # adjust as needed\n",
        "\n",
        "    # Lists to store training metrics\n",
        "    epoch_losses = []\n",
        "    epoch_accuracies = []\n",
        "\n",
        "    # Training loop (simple version)\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (images, labels, bboxes, img_ids) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = alexnet(images)   # shape: (batch_size, 21)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += images.size(0)\n",
        "\n",
        "            if (batch_idx + 1) % 50 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = correct / total * 100\n",
        "        \n",
        "        # Store metrics for plotting\n",
        "        epoch_losses.append(epoch_loss)\n",
        "        epoch_accuracies.append(epoch_acc)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1} finished. Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
        "\n",
        "    # Save fine-tuned weights\n",
        "    torch.save(alexnet.state_dict(), finetuned_model_path)\n",
        "    print(f\"Saved fine-tuned AlexNet weights to '{finetuned_model_path}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generally, the training loss and accuracy look like below.\n",
        "\n",
        "![alexnet_training_graph.png](./screenshots/alexnet_training_graph.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not does_graph_exist:\n",
        "    print(\"Training completed. Now visualizing training metrics...\")\n",
        "    # Visualize training loss and accuracy\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epoch_losses, marker='o')\n",
        "    plt.title('Training Loss per Epoch')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epoch_accuracies, marker='o', color='orange')\n",
        "    plt.title('Training Accuracy per Epoch')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping training visualization since the model was not trained in this run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract and save fc7 features for every warped patch\n",
        "\n",
        "Loop over all warped patches, run through `feature_extractor`, and save 4096-Dim features we’ll process in batches to utilize the GPU. Each feature vector is saved as a NumPy `.npy` file named `{img_id}_{prop_idx}.npy`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload the fine-tuned weights (optional but safe)\n",
        "net = models.alexnet(pretrained=False)\n",
        "net.classifier[6] = nn.Linear(num_ftrs, 21)  # same architecture\n",
        "net.load_state_dict(torch.load('./RCNNDataCache/alexnet_finetuned.pth'))\n",
        "net = net.to(device)\n",
        "net.eval()\n",
        "\n",
        "# Build the feature extractor: up to fc7 (exclude classifier[6])\n",
        "feature_extractor = nn.Sequential(\n",
        "    net.features,         # conv layers\n",
        "    nn.Flatten(),         # flatten conv output\n",
        "    *list(net.classifier[:5])  # fc6 and fc7 layers\n",
        ")\n",
        "feature_extractor = feature_extractor.to(device)\n",
        "feature_extractor.eval()\n",
        "print(\"Feature extractor ready—outputs 4096-dim vectors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. We gather all warped patches (e.g., `['.../000005_0.jpg', '.../000005_1.jpg', …]`) using `glob`\n",
        "2. We apply exactly the same normalization transforms (Resize => ToTensor => Normalize) as during fine-tuning\n",
        "3. Using `torch.no_grad()`, we push a batch of (N,3,227,227) => get (N,4096) features, convert to NumPy, and save each vector uniquely as `{img_id}_{prop_idx}.npy` in `.RCNNDataCache/features/`\n",
        "\n",
        "It takes damn a lot of time and file storages. I bet you had to borrow me a server-grade computer before bombarding this bombshell to me :/\n",
        "\n",
        "![batch_feature_extraction_screenshot.png](./screenshots/batch_feature_extraction_screenshot.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Gather all patch paths\n",
        "patch_list = glob.glob(os.path.join(patches_dir, '*.jpg'))\n",
        "print(f\"Found {len(patch_list)} patch images to extract features from.\")\n",
        "\n",
        "# Define a simple transform that matches how we normalized during fine-tuning\n",
        "feat_transforms = transforms.Compose([\n",
        "    transforms.Resize((227, 227)),                                  # Ensure all patches are resized to 227x227\n",
        "    transforms.ToTensor(),                                          # Convert to tensor\n",
        "    transforms.Normalize(mean=alexnet_mean, std=alexnet_std)        # Normalize as per AlexNet\n",
        "])\n",
        "\n",
        "def batch_feature_extraction(patch_paths, feature_extractor, batch_size=64):\n",
        "    \"\"\"\n",
        "    Extract fc7 features for a list of patch image paths in batches.\n",
        "\n",
        "    Args:\n",
        "        patch_paths (list): List of image file paths.\n",
        "        feature_extractor (nn.Module): Model up to fc7.\n",
        "        batch_size (int): Batch size for processing.\n",
        "\n",
        "    Returns:\n",
        "        features (np.ndarray): Array of shape (N, 4096) with fc7 features.\n",
        "        ids (list): List of (img_id, prop_idx) tuples for each patch.\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    ids = []\n",
        "    for i in range(0, len(patch_paths), batch_size):\n",
        "        batch_files = patch_paths[i:i+batch_size]\n",
        "        batch_images = []\n",
        "        batch_ids = []\n",
        "        for p in batch_files:\n",
        "            # Extract image ID and proposal index from filename\n",
        "            base = os.path.basename(p).split('.')[0]\n",
        "            img_id, prop_idx_str = base.split('_')\n",
        "            prop_idx = int(prop_idx_str)\n",
        "            img = cv2.imread(p)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = transforms.ToPILImage()(img)\n",
        "            img_tensor = feat_transforms(img)\n",
        "            batch_images.append(img_tensor)\n",
        "            batch_ids.append((img_id, prop_idx))\n",
        "        batch_images = torch.stack(batch_images).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            batch_feats = feature_extractor(batch_images)  # shape: (N, 4096)\n",
        "        batch_feats = batch_feats.cpu().numpy()\n",
        "        features.append(batch_feats)\n",
        "        ids.extend(batch_ids)\n",
        "\n",
        "    features = np.vstack(features)  # shape: (total_patches, 4096)\n",
        "    return features, ids\n",
        "\n",
        "# Run extraction and save features for each patch as a .npy file\n",
        "for patch_path in tqdm(patch_list, desc=\"Extracting and saving features\"):\n",
        "    base = os.path.basename(patch_path).split('.')[0]\n",
        "    img_id, prop_idx_str = base.split('_')\n",
        "    out_path = os.path.join('./RCNNDataCache/features', f\"{img_id}_{prop_idx_str}.npy\")\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    if os.path.exists(out_path):\n",
        "        continue  # Skip if feature already exists\n",
        "    \n",
        "    # Extract feature for this patch\n",
        "    img = cv2.imread(patch_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = transforms.ToPILImage()(img)\n",
        "    img_tensor = feat_transforms(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        feat_vec = feature_extractor(img_tensor).cpu().numpy().squeeze()\n",
        "    np.save(out_path, feat_vec)\n",
        "print(\"Saved all fc7 feature vectors to './RCNNDataCache/features/'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train One-vs-All SVMs using `scikit-learn`\n",
        "\n",
        "Load Saved Feature Vectors & Labels, Train One-vs-All `LinearSVC` for each class\n",
        "\n",
        "We’ll read back each feature `.npy` and match it to its label from `rcnn_index.txt`. Then for each of the 20 VOC classes, train a linear SVM: positives are proposals labeled = c, negatives are proposals labeled = 20 (background).\n",
        "\n",
        "Meanwhile, it takes really a lot of RAM... If I had to run this on Colab, I had to pay. And.... it really takes a lot of times!!!\n",
        "\n",
        "![one_vs_all_svm_training_screenshot.png](./screenshots/one_vs_all_svm_training_screenshot.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "# Load index file into memory for fast lookup\n",
        "# index_dict maps (img_id, prop_idx) → label for each proposal patch\n",
        "index_dict = {}\n",
        "with open('./RCNNDataCache/rcnn_index.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        img_id, prop_idx_str, label_str, x, y, w, h = line.strip().split(',')\n",
        "        index_dict[(img_id, int(prop_idx_str))] = int(label_str)\n",
        "\n",
        "# Prepare data structures for SVM training\n",
        "# For each class c in [0..19], gather features and ±1 labels for one-vs-all SVM\n",
        "svm_models = {}\n",
        "feat_files = glob.glob('./RCNNDataCache/features/*.npy')  # List of all feature files\n",
        "\n",
        "for c in range(len(VOC_CLASSES)):\n",
        "    X_pos = []  # Positive samples for class c\n",
        "    X_neg = []  # Negative samples (background) for class c\n",
        "\n",
        "    # If there's already a saved SVM for this class, skip training\n",
        "    if os.path.exists(f'./RCNNDataCache/svm_{VOC_CLASSES[c]}.pkl'):\n",
        "        print(f\"Skipping class {VOC_CLASSES[c]}: SVM already exists.\")\n",
        "        continue\n",
        "\n",
        "    # Iterate over all feature files with progress bar\n",
        "    for ff in tqdm(feat_files, desc=f\"Collecting features for class {VOC_CLASSES[c]}\"):\n",
        "        base = os.path.basename(ff).split('.')[0]\n",
        "        img_id, prop_idx_str = base.split('_')\n",
        "        prop_idx = int(prop_idx_str)\n",
        "        label = index_dict.get((img_id, prop_idx), 20)\n",
        "        feat_vec = np.load(ff)\n",
        "\n",
        "        if label == c:\n",
        "            X_pos.append(feat_vec)\n",
        "        elif label == 20:\n",
        "            X_neg.append(feat_vec)\n",
        "        # Proposals labeled as another positive class are ignored\n",
        "\n",
        "    # Stack features and create ±1 labels for SVM\n",
        "    X_train = np.vstack([X_pos, X_neg])\n",
        "    y_train = np.hstack([np.ones(len(X_pos)), -np.ones(len(X_neg))])\n",
        "    print(f\"Class {VOC_CLASSES[c]}: {len(X_pos)} positives, {len(X_neg)} negatives\")\n",
        "\n",
        "    # Train LinearSVC for class c (one-vs-all)\n",
        "    svm = LinearSVC(C=0.01, max_iter=10000, verbose=1)\n",
        "    svm.fit(X_train, y_train)\n",
        "\n",
        "    # Save trained SVM model for this class\n",
        "    joblib.dump(svm, f'./RCNNDataCache/svm_{VOC_CLASSES[c]}.pkl')\n",
        "    print(f\"Trained and saved SVM for class {VOC_CLASSES[c]}\")\n",
        "    svm_models[c] = svm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit a `LinearRegression` (4-D) for Each Class to Refine Proposal Boxes\n",
        "\n",
        "For every positive example (IoU ≥ 0.5 to a GT box of class c), compute the regression targets `(tx, ty, tw, th)` as in the R-CNN paper. Then train one `LinearRegression` per class to map 4096-dim features => 4-d offsets.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility: Compute regression targets for a proposal vs. its best GT\n",
        "def compute_regression_target(prop_box, gt_box):\n",
        "    \"\"\"\n",
        "    Given prop_box = (xmin_p, ymin_p, xmax_p, ymax_p) and gt_box = (xmin_gt, ymin_gt, xmax_gt, ymax_gt),\n",
        "    return (tx, ty, tw, th) offsets per R-CNN:\n",
        "      t_x = (x_gt - x_p) / w_p,  t_y = (y_gt - y_p) / h_p,\n",
        "      t_w = log(w_gt / w_p),     t_h = log(h_gt / h_p).\n",
        "    Here (x_p, y_p) are centers of proposal, same for ground-truth.\n",
        "    \"\"\"\n",
        "    xmin_p, ymin_p, xmax_p, ymax_p = prop_box\n",
        "    xmin_gt, ymin_gt, xmax_gt, ymax_gt = gt_box\n",
        "\n",
        "    w_p = xmax_p - xmin_p + 1\n",
        "    h_p = ymax_p - ymin_p + 1\n",
        "    x_p = xmin_p + 0.5 * w_p\n",
        "    y_p = ymin_p + 0.5 * h_p\n",
        "\n",
        "    w_gt = xmax_gt - xmin_gt + 1\n",
        "    h_gt = ymax_gt - ymin_gt + 1\n",
        "    x_gt = xmin_gt + 0.5 * w_gt\n",
        "    y_gt = ymin_gt + 0.5 * h_gt\n",
        "\n",
        "    t_x = (x_gt - x_p) / w_p\n",
        "    t_y = (y_gt - y_p) / h_p\n",
        "    t_w = np.log(w_gt / w_p)\n",
        "    t_h = np.log(h_gt / h_p)\n",
        "    return (t_x, t_y, t_w, t_h)\n",
        "\n",
        "# For each class c, gather (feature_vec, target_offsets)\n",
        "reg_models = {}\n",
        "for c in range(len(VOC_CLASSES)):\n",
        "    X_regs = []\n",
        "    Y_regs = []\n",
        "\n",
        "    # Read all index entries and match to .npy features\n",
        "    with open('./RCNNDataCache/rcnn_index.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            img_id, prop_idx_str, label_str, x_str, y_str, w_str, h_str = line.strip().split(',')\n",
        "            prop_idx = int(prop_idx_str)\n",
        "            label = int(label_str)\n",
        "            if label != c:\n",
        "                continue  # only positives for class c\n",
        "\n",
        "            # Compute box coordinates\n",
        "            x, y, w, h = map(int, [x_str, y_str, w_str, h_str])\n",
        "            prop_box = (x, y, x + w, y + h)\n",
        "\n",
        "            # Parse GT to find the exact GT box that gave IoU >= 0.5 (positive sample)\n",
        "            xml_path = os.path.join(annotations_dir, f\"{img_id}.xml\")\n",
        "            gt_objs = parse_voc_annotation(xml_path)\n",
        "            best_iou = 0.0\n",
        "            best_gt_box = None\n",
        "            for obj in gt_objs:\n",
        "                if CLASS_TO_IDX[obj['class']] != c:\n",
        "                    continue\n",
        "                gt_box = obj['bbox']  # (xmin,ymin,xmax,ymax)\n",
        "                iou = compute_iou(prop_box, gt_box)\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_box = gt_box\n",
        "\n",
        "            if best_gt_box is None:\n",
        "                continue\n",
        "\n",
        "            # Compute regression target\n",
        "            t = compute_regression_target(prop_box, best_gt_box)  # (tx,ty,tw,th)\n",
        "\n",
        "            # Load feature vector\n",
        "            feat_path = os.path.join('./RCNNDataCache/features', f\"{img_id}_{prop_idx}.npy\")\n",
        "            feat_vec = np.load(feat_path)\n",
        "            X_regs.append(feat_vec)\n",
        "            Y_regs.append(t)  # collect 4-d tuple (tx, ty, tw, th)\n",
        "\n",
        "    if not X_regs:\n",
        "        print(f\"No positive samples for class {VOC_CLASSES[c]}; skipping regressor.\")\n",
        "        continue\n",
        "\n",
        "    X_regs = np.vstack(X_regs)      # shape: (N_pos, 4096)\n",
        "    Y_regs = np.vstack(Y_regs)      # shape: (N_pos, 4)\n",
        "\n",
        "    # Fit LinearRegression\n",
        "    reg = LinearRegression()\n",
        "    reg.fit(X_regs, Y_regs)\n",
        "    joblib.dump(reg, f'./RCNNDataCache/bbox_reg_{VOC_CLASSES[c]}.pkl')\n",
        "    print(f\"Trained and saved bbox regressor for class {VOC_CLASSES[c]} with {len(X_regs)} samples.\")\n",
        "    reg_models[c] = reg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build `proposals_test_dict` for VOC 2007 TEST images (50 proposals each) and Warp to (227, 227). We reuse the same Selective Search pipeline, limiting to the top 50 regions, then warp and save each cropped proposal as `{img_id}_{prop_idx}.jpg` under `./RCNNDataCache/warped_patches_test/`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import multiprocessing as mp\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Path definitions\n",
        "JPEG_DIR = './VOCdevkit/VOC2007/JPEGImages'\n",
        "TEST_IDS_FILE = './VOCdevkit/VOC2007/ImageSets/Main/test.txt'           # one ID per line without .jpg\n",
        "proposals_test_dict = {}  \n",
        "output_test_patches = './RCNNDataCache/warped_patches_test/'\n",
        "os.makedirs(output_test_patches, exist_ok=True)\n",
        "\n",
        "# Load TEST image IDs, or create the file if it does not exist\n",
        "if not os.path.exists(TEST_IDS_FILE):\n",
        "    # List all .jpg files in JPEG_DIR and write their names (without .jpg) to TEST_IDS_FILE\n",
        "    image_files = sorted([f for f in os.listdir(JPEG_DIR) if f.lower().endswith('.jpg')])\n",
        "    with open(TEST_IDS_FILE, 'w') as f:\n",
        "        for img_file in image_files:\n",
        "            img_id = os.path.splitext(img_file)[0]\n",
        "            f.write(f\"{img_id}\\n\")\n",
        "    print(f\"Created {TEST_IDS_FILE} with {len(image_files)} image IDs.\")\n",
        "\n",
        "with open(TEST_IDS_FILE, 'r') as f:\n",
        "    test_ids = [line.strip() for line in f if line.strip()]\n",
        "print(f\"Found {len(test_ids)} test images in VOC 2007 test split.\")\n",
        "\n",
        "# Initialize Selective Search once per worker\n",
        "_ss_for_test = None\n",
        "def init_ss_test():\n",
        "    global _ss_for_test\n",
        "    _ss_for_test = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
        "\n",
        "def compute_test_proposals(img_id):\n",
        "    \"\"\"\n",
        "    Worker: load image, run SS (Fast), keep top 50, filter w/h >= 20, return (img_id, filtered_boxes).\n",
        "    \"\"\"\n",
        "    global _ss_for_test\n",
        "    img_path = os.path.join(JPEG_DIR, f\"{img_id}.jpg\")\n",
        "    image = cv2.imread(img_path)\n",
        "    if image is None:\n",
        "        return (img_id, [])\n",
        "    _ss_for_test.setBaseImage(image)\n",
        "    _ss_for_test.switchToSelectiveSearchFast()\n",
        "    rects = _ss_for_test.process()\n",
        "    top50 = rects[:50]\n",
        "    filtered = [r for r in top50 if r[2] >= 20 and r[3] >= 20]\n",
        "    return (img_id, filtered)\n",
        "\n",
        "# Parallelize SS over TEST images with progress reporting\n",
        "num_workers = mp.cpu_count() - 3 or 1\n",
        "results = []\n",
        "with mp.Pool(processes=num_workers, initializer=init_ss_test) as pool:\n",
        "    for idx, res in enumerate(pool.imap(compute_test_proposals, test_ids), 1):\n",
        "        results.append(res)\n",
        "        percent = (idx / len(test_ids)) * 100\n",
        "        print(f\"{idx} / {len(test_ids)} test images processed... {res[0]}: {len(res[1])} proposals ({percent:.2f}%)\")\n",
        "\n",
        "proposals_test_dict = {img_id: boxes for (img_id, boxes) in results}\n",
        "print(f\"Collected proposals for {len(proposals_test_dict)} test images (≤50 each).\")\n",
        "\n",
        "# Warp & Save each test proposal to 227×227\n",
        "def warp_and_save_patch(image, box, out_path):\n",
        "    \"\"\"\n",
        "    Crop ROI from `image` using (x,y,w,h), resize to (227,227), and save to out_path.\n",
        "    Returns True if saved, False otherwise.\n",
        "    \"\"\"\n",
        "    x, y, w, h = box\n",
        "    x2 = min(x + w, image.shape[1] - 1)\n",
        "    y2 = min(y + h, image.shape[0] - 1)\n",
        "    crop = image[y : y2, x : x2]\n",
        "    if crop is None or crop.size == 0:\n",
        "        return False\n",
        "    resized = cv2.resize(crop, (227, 227), interpolation=cv2.INTER_LINEAR)\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    return cv2.imwrite(out_path, resized)\n",
        "\n",
        "# Loop through proposals_test_dict and save warped patches\n",
        "for img_id, boxes in tqdm(proposals_test_dict.items(), desc=\"Warping test proposals\"):\n",
        "    image = cv2.imread(os.path.join(JPEG_DIR, f\"{img_id}.jpg\"))\n",
        "    if image is None:\n",
        "        continue\n",
        "    for idx, box in enumerate(boxes):\n",
        "        out_name = f\"{img_id}_{idx}.jpg\"\n",
        "        out_path = os.path.join(output_test_patches, out_name)\n",
        "        warp_and_save_patch(image, box, out_path)\n",
        "\n",
        "print(\"Finished warping and saving test proposals to './RCNNDataCache/warped_patches_test/'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Batch-Extract 4096-Dim fc7 features using the fine-tuned AlexNet\n",
        "\n",
        "Reuse our `feature_extractor` (AlexNet up to fc7), perform inference on every test patch in batches, and save each feature vector as `{img_id}_{prop_idx}.npy` under `./RCNNDataCache/features_test/`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Gather all test patch paths\n",
        "test_patch_list = glob.glob(os.path.join(output_test_patches, '*.jpg'))\n",
        "print(f\"Found {len(test_patch_list)} test patch images for feature extraction.\")\n",
        "\n",
        "# Check if features already exist for all test patches\n",
        "features_dir = './RCNNDataCache/features_test/'\n",
        "os.makedirs(features_dir, exist_ok=True)\n",
        "all_exist = all(\n",
        "    os.path.exists(os.path.join(features_dir, os.path.basename(p).replace('.jpg', '.npy')))\n",
        "    for p in test_patch_list\n",
        ")\n",
        "if all_exist:\n",
        "    print(\"All test features already extracted. Skipping extraction.\")\n",
        "else:\n",
        "    print(\"Some test features are missing. Proceeding with extraction...\")\n",
        "\n",
        "def extract_and_save_features(patch_paths, feat_extractor, batch_size=64):\n",
        "    \"\"\"\n",
        "    Given a list of patch_paths, run them through feat_extractor in batches.\n",
        "    Save each feature vector as .npy in './RCNNDataCache/features_test/'.\n",
        "    \"\"\"\n",
        "    os.makedirs('./RCNNDataCache/features_test/', exist_ok=True)\n",
        "    for i in tqdm(range(0, len(patch_paths), batch_size), desc=\"Extracting features\"):\n",
        "        batch_files = patch_paths[i : i + batch_size]\n",
        "        batch_imgs = []\n",
        "        batch_ids = []\n",
        "\n",
        "        for p in batch_files:\n",
        "            base = os.path.basename(p).split('.')[0]  # example) '000005_12'\n",
        "            img_id, prop_idx_str = base.split('_')\n",
        "            prop_idx = int(prop_idx_str)\n",
        "            img_bgr = cv2.imread(p)\n",
        "            img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "            img_pil = transforms.ToPILImage()(img_rgb)\n",
        "            img_tensor = feat_transforms(img_pil)\n",
        "            batch_imgs.append(img_tensor)\n",
        "            batch_ids.append((img_id, prop_idx))\n",
        "\n",
        "        batch_tensor = torch.stack(batch_imgs).to(device)\n",
        "        with torch.no_grad():\n",
        "            feats = feat_extractor(batch_tensor)  # shape: (N,4096)\n",
        "        feats_np = feats.cpu().numpy()\n",
        "        for vec, (img_id, prop_idx) in zip(feats_np, batch_ids):\n",
        "            save_path = f'./RCNNDataCache/features_test/{img_id}_{prop_idx}.npy'\n",
        "            np.save(save_path, vec)\n",
        "\n",
        "extract_and_save_features(test_patch_list, feature_extractor, batch_size=64)\n",
        "print(\"Saved all test fc7 features to './RCNNDataCache/features_test/'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load All SVMs (`./RCNNDataCache/svm_{classname}.pkl`) and BBox Regressors (`./RCNNDataCache/bbox_reg_{classname}.pkl`), Then:\n",
        "\n",
        "1. For each test proposal feature vector:  \n",
        "   - For each class **c** (0–19; 20 VoC classes total), compute `score_c = svm_models[c].decision_function(feat_vec)`.  \n",
        "   - If `score_c` <= `score_threshold_c` (we’ll use 0.0 or –1.0 by default), skip detection for this class.  \n",
        "   - Otherwise, load that class’s regressor, compute `(tx, ty, tw, th) = reg.predict(feat_vec)`, adjust the original proposal `(x,y,w,h)` to get a refined box `(xmin', ymin', xmax', ymax')`.  \n",
        "2. Collect all `(img_id, class_c, score_c, refined_box)` entries in a list `all_detections`.  \n",
        "\n",
        "- - -\n",
        "- We load each class’s LinearSVC and LinearRegression from disk into memory to avoid repeated I/O\n",
        "- The R-CNN paper suggests a score threshold around 0.0 or –1.0; here we use 0.0 by default to reduce false positives (lower recall). You can experiment with a lower threshold if needed\n",
        "- Computing (tx,ty,tw,th) and applying them exactly as in the original R-CNN gives refined boxes aligned to objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "from tqdm import tqdm\n",
        "\n",
        "svm_models = {}\n",
        "regressors = {}\n",
        "for idx, cls_name in enumerate(VOC_CLASSES):\n",
        "    svm_models[idx] = joblib.load(f'./RCNNDataCache/svm_{cls_name}.pkl')\n",
        "    regressors[idx] = joblib.load(f'./RCNNDataCache/bbox_reg_{cls_name}.pkl')\n",
        "\n",
        "# Helper: Convert proposal (x,y,w,h) to (xmin,ymin,xmax,ymax)\n",
        "def prop_to_xyxy(box):\n",
        "    x, y, w, h = box\n",
        "    return (x, y, x + w, y + h)\n",
        "\n",
        "# Helper: Apply (tx,ty,tw,th) to (xmin,ymin,xmax,ymax)\n",
        "def apply_regression(orig_box, offsets):\n",
        "    \"\"\"\n",
        "    orig_box: (xmin, ymin, xmax, ymax)\n",
        "    offsets: (tx, ty, tw, th)\n",
        "    Returns refined (xmin', ymin', xmax', ymax') as ints.\n",
        "    \"\"\"\n",
        "    # Use formula from R-CNN:\n",
        "    xmin, ymin, xmax, ymax = orig_box\n",
        "    w = xmax - xmin + 1\n",
        "    h = ymax - ymin + 1\n",
        "    cx = xmin + 0.5 * w\n",
        "    cy = ymin + 0.5 * h\n",
        "\n",
        "    tx, ty, tw, th = offsets\n",
        "    cx_p = tx * w + cx\n",
        "    cy_p = ty * h + cy\n",
        "    w_p = np.exp(tw) * w\n",
        "    h_p = np.exp(th) * h\n",
        "\n",
        "    xmin_p = int(cx_p - 0.5 * w_p)\n",
        "    ymin_p = int(cy_p - 0.5 * h_p)\n",
        "    xmax_p = int(cx_p + 0.5 * w_p)\n",
        "    ymax_p = int(cy_p + 0.5 * h_p)\n",
        "    return (xmin_p, ymin_p, xmax_p, ymax_p)\n",
        "\n",
        "# Collect all test detections\n",
        "all_detections = []  # list of dicts: {'img_id', 'class_idx', 'score', 'bbox'}\n",
        "\n",
        "# Iterate over every test image and its proposals\n",
        "for img_id, boxes in tqdm(proposals_test_dict.items(), desc=\"Scoring test proposals\"):\n",
        "    for prop_idx, box in enumerate(boxes):\n",
        "        feat_path = f'./RCNNDataCache/features_test/{img_id}_{prop_idx}.npy'\n",
        "        if not os.path.exists(feat_path):\n",
        "            continue\n",
        "\n",
        "        feat_vec = np.load(feat_path).reshape(1, -1)  # shape: (1,4096)\n",
        "        orig_xyxy = prop_to_xyxy(box)\n",
        "        \n",
        "        for c in range(len(VOC_CLASSES)):\n",
        "            svm = svm_models[c]\n",
        "            score_c = svm.decision_function(feat_vec)[0]  # scalar\n",
        "\n",
        "            # Threshold: keep score > 0.0 (you can tune to –1.0 for more recall)\n",
        "            if score_c <= 0.0:\n",
        "                continue\n",
        "            \n",
        "            # Refinement\n",
        "            reg = regressors[c]\n",
        "            offsets = reg.predict(feat_vec)[0]  # (tx, ty, tw, th)\n",
        "            refined_box = apply_regression(orig_xyxy, offsets)\n",
        "            all_detections.append({\n",
        "                'img_id': img_id,\n",
        "                'class_idx': c,\n",
        "                'score': float(score_c),\n",
        "                'bbox': refined_box\n",
        "            })\n",
        "\n",
        "print(f\"Total raw detections (before NMS): {len(all_detections)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply Standard NMS (IoU <= 0.3) for Each Class\n",
        "\n",
        "Group `all_detections` by `class_idx`, run NMS (threshold 0.3), and produce a final list `final_detections`.  \n",
        "\n",
        "> NMS is a greedy algorithm that loops over all the classes, and for each class it checks for overlaps (IoU — Intersection over Union) between all the bounding boxes. If the IoU between two boxes of the same class is above a certain threshold (usually 0.7), the algorithm concludes that they refer to the same object, and discards the box with the lower confidence score (which is a product of the objectness score and the conditional class probability).\n",
        "\n",
        "- We sort each class’s detections by descending score, then greedily remove any detection whose IoU > 0.3 with a higher-scoring box\n",
        "- After NMS, we get the final set of output boxes per image & per class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Helper: Compute IoU of two boxes (xmin,ymin,xmax,ymax)\n",
        "def compute_iou_xyxy(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "\n",
        "    interW = max(0, xB - xA + 1)\n",
        "    interH = max(0, yB - yA + 1)\n",
        "    interArea = interW * interH\n",
        "\n",
        "    areaA = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
        "    areaB = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
        "    union = areaA + areaB - interArea\n",
        "\n",
        "    if union == 0:\n",
        "        return 0.0\n",
        "    return interArea / union\n",
        "\n",
        "# Standard NMS function\n",
        "def nms_single_class(detections, iou_thresh=0.3):\n",
        "    \"\"\"\n",
        "    detections: list of dicts with keys ['bbox', 'score'] for a single class.\n",
        "    Returns a subset of detections after NMS.\n",
        "    \"\"\"\n",
        "    if not detections:\n",
        "        return []\n",
        "    \n",
        "    # Sort by descending score\n",
        "    detections = sorted(detections, key=lambda x: x['score'], reverse=True)\n",
        "    keep = []\n",
        "    while detections:\n",
        "        best = detections.pop(0)\n",
        "        keep.append(best)\n",
        "        filtered = []\n",
        "        for det in detections:\n",
        "            iou = compute_iou_xyxy(best['bbox'], det['bbox'])\n",
        "            if iou <= iou_thresh:\n",
        "                filtered.append(det)\n",
        "        detections = filtered\n",
        "    return keep\n",
        "\n",
        "# Run NMS for all classes\n",
        "final_detections = []  # list of dicts: {'img_id','class_idx','score','bbox'}\n",
        "\n",
        "for c in tqdm(range(len(VOC_CLASSES)), desc=\"NMS per class\"):\n",
        "    # Filter raw detections to only class c\n",
        "    cls_dets = [d for d in all_detections if d['class_idx'] == c]\n",
        "    keep_cls = nms_single_class(cls_dets, iou_thresh=0.3)\n",
        "    final_detections.extend(keep_cls)\n",
        "    print(f\"Class {VOC_CLASSES[c]}: raw {len(cls_dets)} → NMS {len(keep_cls)}\")\n",
        "\n",
        "print(f\"Total detections after NMS: {len(final_detections)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save Final Detections to `./RCNNDataCache/results/` in VOC’s Required Text Format\n",
        "\n",
        "Pascal VOC expects, for each class **c**, a file named `comp4_det_test_{classname}.txt` containing lines:\n",
        "```\n",
        "<image_id> <confidence> <xmin> <ymin> <xmax> <ymax>\n",
        "```\n",
        "in ascending order of `image_id`. We will create a folder `./RCNNDataCache/results/` and populate one file per class.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Prepare result directory\n",
        "results_dir = './RCNNDataCache/results/'\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Initialize a dict to accumulate lines per class\n",
        "detection_lines = {c: [] for c in range(len(VOC_CLASSES))}\n",
        "\n",
        "# Populate lines from final_detections\n",
        "for det in tqdm(final_detections, desc=\"Formatting detections\"):\n",
        "    img_id = det['img_id']\n",
        "    c = det['class_idx']\n",
        "    score = det['score']\n",
        "    xmin, ymin, xmax, ymax = det['bbox']\n",
        "    line = f\"{img_id} {score:.6f} {xmin} {ymin} {xmax} {ymax}\\n\"\n",
        "    detection_lines[c].append(line)\n",
        "\n",
        "# Sort lines by image_id (lexicographically)\n",
        "for c in tqdm(range(len(VOC_CLASSES)), desc=\"Writing result files\"):\n",
        "    detection_lines[c].sort(key=lambda x: x.split()[0])  # sorts by img_id string\n",
        "    outfile = os.path.join(results_dir, f\"comp4_det_test_{VOC_CLASSES[c]}.txt\")\n",
        "    \n",
        "    with open(outfile, 'w') as f:\n",
        "        f.writelines(detection_lines[c])\n",
        "    print(f\"Wrote {len(detection_lines[c])} detections to {outfile}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run Pascal VOC 2007 Official mAP Evaluation\n",
        "\n",
        "Use the Official Pascal VOC `voc_eval.py` Script to Compute AP per Class and mAP We will call `voc_eval.py` (shipped with Pascal VOC devkit) to parse our detection files vs. ground-truth annotations in `VOC2007/Annotations`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------------------------\n",
        "# Fast/er R-CNN\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Written by Bharath Hariharan\n",
        "# --------------------------------------------------------\n",
        "# Source code: https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/datasets/voc_eval.py\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import _pickle as cPickle\n",
        "import numpy as np\n",
        "\n",
        "def parse_rec(filename):\n",
        "    \"\"\" Parse a PASCAL VOC xml file \"\"\"\n",
        "    tree = ET.parse(filename)\n",
        "    objects = []\n",
        "    for obj in tree.findall('object'):\n",
        "        obj_struct = {}\n",
        "        obj_struct['name'] = obj.find('name').text\n",
        "        obj_struct['pose'] = obj.find('pose').text\n",
        "        obj_struct['truncated'] = int(obj.find('truncated').text)\n",
        "        obj_struct['difficult'] = int(obj.find('difficult').text)\n",
        "        bbox = obj.find('bndbox')\n",
        "        obj_struct['bbox'] = [int(bbox.find('xmin').text),\n",
        "                              int(bbox.find('ymin').text),\n",
        "                              int(bbox.find('xmax').text),\n",
        "                              int(bbox.find('ymax').text)]\n",
        "        objects.append(obj_struct)\n",
        "\n",
        "    return objects\n",
        "\n",
        "def voc_ap(rec, prec, use_07_metric=False):\n",
        "    \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n",
        "    Compute VOC AP given precision and recall.\n",
        "    If use_07_metric is true, uses the\n",
        "    VOC 07 11 point method (default:False).\n",
        "    \"\"\"\n",
        "    if use_07_metric:\n",
        "        # 11 point metric\n",
        "        ap = 0.\n",
        "        for t in np.arange(0., 1.1, 0.1):\n",
        "            if np.sum(rec >= t) == 0:\n",
        "                p = 0\n",
        "            else:\n",
        "                p = np.max(prec[rec >= t])\n",
        "            ap = ap + p / 11.\n",
        "    else:\n",
        "        # correct AP calculation\n",
        "        # first append sentinel values at the end\n",
        "        mrec = np.concatenate(([0.], rec, [1.]))\n",
        "        mpre = np.concatenate(([0.], prec, [0.]))\n",
        "\n",
        "        # compute the precision envelope\n",
        "        for i in range(mpre.size - 1, 0, -1):\n",
        "            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
        "\n",
        "        # to calculate area under PR curve, look for points\n",
        "        # where X axis (recall) changes value\n",
        "        i = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "\n",
        "        # and sum (\\Delta recall) * prec\n",
        "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
        "    return ap\n",
        "\n",
        "def voc_eval(detpath,\n",
        "             annopath,\n",
        "             imagesetfile,\n",
        "             classname,\n",
        "             cachedir,\n",
        "             ovthresh=0.5,\n",
        "             use_07_metric=False):\n",
        "    \"\"\"rec, prec, ap = voc_eval(detpath,\n",
        "                                annopath,\n",
        "                                imagesetfile,\n",
        "                                classname,\n",
        "                                [ovthresh],\n",
        "                                [use_07_metric])\n",
        "\n",
        "    Top level function that does the PASCAL VOC evaluation.\n",
        "\n",
        "    detpath: Path to detections\n",
        "        detpath.format(classname) should produce the detection results file.\n",
        "    annopath: Path to annotations\n",
        "        annopath.format(imagename) should be the xml annotations file.\n",
        "    imagesetfile: Text file containing the list of images, one image per line.\n",
        "    classname: Category name (duh)\n",
        "    cachedir: Directory for caching the annotations\n",
        "    [ovthresh]: Overlap threshold (default = 0.5)\n",
        "    [use_07_metric]: Whether to use VOC07's 11 point AP computation\n",
        "        (default False)\n",
        "    \"\"\"\n",
        "    # assumes detections are in detpath.format(classname)\n",
        "    # assumes annotations are in annopath.format(imagename)\n",
        "    # assumes imagesetfile is a text file with each line an image name\n",
        "    # cachedir caches the annotations in a pickle file\n",
        "\n",
        "    # first load gt\n",
        "    if not os.path.isdir(cachedir):\n",
        "        os.mkdir(cachedir)\n",
        "    cachefile = os.path.join(cachedir, 'annots.pkl')\n",
        "    # read list of images\n",
        "    with open(imagesetfile, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    imagenames = [x.strip() for x in lines]\n",
        "\n",
        "    if not os.path.isfile(cachefile):\n",
        "        # load annots\n",
        "        recs = {}\n",
        "        for i, imagename in enumerate(imagenames):\n",
        "            recs[imagename] = parse_rec(annopath.format(imagename))\n",
        "            if i % 100 == 0:\n",
        "                print('Reading annotation for {}/{}'.format(\n",
        "                    i + 1, len(imagenames)))\n",
        "        # save\n",
        "        print('Saving cached annotations to {}'.format(cachefile))\n",
        "        with open(cachefile, 'wb') as f:\n",
        "            cPickle.dump(recs, f)\n",
        "    else:\n",
        "        # load\n",
        "        with open(cachefile, 'rb') as f:\n",
        "            recs = cPickle.load(f)\n",
        "\n",
        "    # extract gt objects for this class\n",
        "    class_recs = {}\n",
        "    npos = 0\n",
        "    for imagename in imagenames:\n",
        "        R = [obj for obj in recs[imagename] if obj['name'] == classname]\n",
        "        bbox = np.array([x['bbox'] for x in R])\n",
        "        difficult = np.array([x['difficult'] for x in R]).astype(np.bool)\n",
        "        det = [False] * len(R)\n",
        "        npos = npos + sum(~difficult)\n",
        "        class_recs[imagename] = {'bbox': bbox,\n",
        "                                 'difficult': difficult,\n",
        "                                 'det': det}\n",
        "\n",
        "    # read dets\n",
        "    detfile = detpath.format(classname)\n",
        "    with open(detfile, 'rb') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # decode bytes to str before splitting\n",
        "    splitlines = [x.decode('utf-8').strip().split(' ') for x in lines]\n",
        "    image_ids = [x[0] for x in splitlines]\n",
        "    confidence = np.array([float(x[1]) for x in splitlines])\n",
        "    BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n",
        "\n",
        "    # sort by confidence\n",
        "    sorted_ind = np.argsort(-confidence)\n",
        "    sorted_scores = np.sort(-confidence)\n",
        "    BB = BB[sorted_ind, :]\n",
        "    image_ids = [image_ids[x] for x in sorted_ind]\n",
        "\n",
        "    # go down dets and mark TPs and FPs\n",
        "    nd = len(image_ids)\n",
        "    tp = np.zeros(nd)\n",
        "    fp = np.zeros(nd)\n",
        "    for d in range(nd):\n",
        "        R = class_recs[image_ids[d]]\n",
        "        bb = BB[d, :].astype(float)\n",
        "        ovmax = -np.inf\n",
        "        BBGT = R['bbox'].astype(float)\n",
        "\n",
        "        if BBGT.size > 0:\n",
        "            # compute overlaps\n",
        "            # intersection\n",
        "            ixmin = np.maximum(BBGT[:, 0], bb[0])\n",
        "            iymin = np.maximum(BBGT[:, 1], bb[1])\n",
        "            ixmax = np.minimum(BBGT[:, 2], bb[2])\n",
        "            iymax = np.minimum(BBGT[:, 3], bb[3])\n",
        "            iw = np.maximum(ixmax - ixmin + 1., 0.)\n",
        "            ih = np.maximum(iymax - iymin + 1., 0.)\n",
        "            inters = iw * ih\n",
        "\n",
        "            # union\n",
        "            uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n",
        "                   (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n",
        "                   (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n",
        "\n",
        "            overlaps = inters / uni\n",
        "            ovmax = np.max(overlaps)\n",
        "            jmax = np.argmax(overlaps)\n",
        "\n",
        "        if ovmax > ovthresh:\n",
        "            if not R['difficult'][jmax]:\n",
        "                if not R['det'][jmax]:\n",
        "                    tp[d] = 1.\n",
        "                    R['det'][jmax] = 1\n",
        "                else:\n",
        "                    fp[d] = 1.\n",
        "        else:\n",
        "            fp[d] = 1.\n",
        "\n",
        "    # compute precision recall\n",
        "    fp = np.cumsum(fp)\n",
        "    tp = np.cumsum(tp)\n",
        "    rec = tp / float(npos)\n",
        "    # avoid divide by zero in case the first detection matches a difficult\n",
        "    # ground truth\n",
        "    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
        "    ap = voc_ap(rec, prec, use_07_metric)\n",
        "\n",
        "    return rec, prec, ap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "cache_dir = './RCNNDataCache/annotations_cache/'\n",
        "cache_file = os.path.join(cache_dir, 'annots.pkl')\n",
        "\n",
        "if os.path.isfile(cache_file) and os.path.getsize(cache_file) == 0:\n",
        "    os.remove(cache_file)\n",
        "    print(f\"Removed empty cache file: {cache_file}\")\n",
        "elif os.path.isfile(cache_file):\n",
        "    print(f\"Cache file {cache_file} exists and is non‐empty (size: {os.path.getsize(cache_file)} bytes).\")\n",
        "else:\n",
        "    print(f\"No cache file found at: {cache_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "VOC_ROOT = './VOCdevkit/VOC2007'\n",
        "ANNOPATH = os.path.join(VOC_ROOT, 'Annotations', '{}.xml')\n",
        "DETPATH  = './RCNNDataCache/results/comp4_det_test_{}.txt'\n",
        "IMAGESETFILE = os.path.join(VOC_ROOT, 'ImageSets', 'Main', 'test.txt')\n",
        "CACHEDIR = './RCNNDataCache/annotations_cache/'\n",
        "os.makedirs(CACHEDIR, exist_ok=True)\n",
        "\n",
        "aps = []\n",
        "for c, cls_name in enumerate(VOC_CLASSES):\n",
        "    rec, prec, ap = voc_eval(\n",
        "        DETPATH,\n",
        "        ANNOPATH,\n",
        "        IMAGESETFILE,\n",
        "        cls_name,\n",
        "        CACHEDIR,\n",
        "        ovthresh=0.5,\n",
        "        use_07_metric=True\n",
        "    )\n",
        "    aps.append(ap)\n",
        "    print(f\"AP for {cls_name:12s} = {ap:.4f}\")\n",
        "\n",
        "mAP = sum(aps) / len(aps)\n",
        "print(f\"\\n===== VOC2007 mAP (50 proposals) = {mAP:.4f} =====\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2, matplotlib.pyplot as plt\n",
        "from random import choice\n",
        "\n",
        "# Pick a class where you at least see some lines in the result file, e.g. \"person\"\n",
        "cls = 'person'\n",
        "dets = []\n",
        "with open(f'RCNNDataCache/results/comp4_det_test_{cls}.txt') as f:\n",
        "    for line in f:\n",
        "        img_id, score, xmin, ymin, xmax, ymax = line.split()\n",
        "        dets.append((img_id, float(score), int(xmin), int(ymin), int(xmax), int(ymax)))\n",
        "\n",
        "# Pick a random detection\n",
        "img_id, score, x1, y1, x2, y2 = choice(dets)\n",
        "img = cv2.imread(f'VOCdevkit/VOC2007/JPEGImages/{img_id}.jpg')\n",
        "cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.title(f\"{cls} | score={score:.2f} | image={img_id}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
